{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering, Scaling, and Cross Validation\n",
    "\n",
    "## Feature Engineering\n",
    "\n",
    "### Dealing with Categorical Features\n",
    "\n",
    "Every machine learning process starts with a data set from which you wish to extract information. Feature engineering lies at the beginning of this process. It deals with cleaning data and the extraction or creation of features from the data set in order to facilitate the prediction of a response. The tools for data cleaning we discussed in the first three chapters. Thankfully, the data sets we use in this course are mostly already cleansed but you should be aware that in practice this is what consumes a significant amount of time. As for the feature extraction part, we saw an example of it in the chapter on KNN where we downloaded share prices and \"generated\" features (i.e. lagged returns) out of it. In that setup all of the feature values were of numerical kind. In the chapters on logistic regression, LDA and QDA we worked with the 'Default' data set. This set had a **binary categorical feature** (e.g. student: yes/no). In order to work with these string values we simply used Pandas' `factorize()` function. Here's the code we applied:\n",
    "\n",
    "`# Factorize 'No' and 'Yes' in columns 'default' and 'student'\n",
    "df['defaultFac'] = df.default.factorize()[0]\n",
    "df['studentFac'] = df.student.factorize()[0]`\n",
    "\n",
    "For the case of binary categories this works perfectly fine. But what if the number of categories is greater than two? How should we deal with it? To illustrate this, imagine the following sample data set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dist</th>\n",
       "      <th>fab</th>\n",
       "      <th>price</th>\n",
       "      <th>rooms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>bricks</td>\n",
       "      <td>1390000</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>concrete</td>\n",
       "      <td>1300000</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>wood</td>\n",
       "      <td>840000</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>concrete</td>\n",
       "      <td>1400000</td>\n",
       "      <td>5.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dist       fab    price  rooms\n",
       "0     5    bricks  1390000    3.5\n",
       "1     2  concrete  1300000    4.5\n",
       "2    12      wood   840000    3.5\n",
       "3     9  concrete  1400000    5.5"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = [{'price': 1390000, 'rooms': 3.5, 'dist': 5, 'fab': 'bricks'},\n",
    "        {'price': 1300000, 'rooms': 4.5, 'dist': 2, 'fab': 'concrete'},\n",
    "        {'price':  840000, 'rooms': 3.5, 'dist': 12, 'fab': 'wood'},\n",
    "        {'price': 1400000, 'rooms': 5.5, 'dist': 9, 'fab': 'concrete'}]\n",
    "data = pd.DataFrame(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we deal with the 'fab' column? Easy, you might think: we just use a numerical mapping, for example 0 = bricks, 1 = concrete, 2 = wood. This is precisely what the output of `pd.factorize()` would be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 1], dtype=int64),\n",
       " Index(['bricks', 'concrete', 'wood'], dtype='object'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.factorize(data['fab'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, if we would follow through with this approach and feed these values into a scikit-learn ML function, the model would make the fundamental assumption that bricks > concrete > wood. Furthermore, we have numerical values for `dist` (the districts) where we know precisely that these represent categorical values (2 = Wollishofen, 5 = Industrie, 9 = Altstetten, 12 = Schwamendingen). And this - geographic or demographic jokes aside - would not make much sense (VanderPlas (2016)). Houston we've got a problem! \n",
    "\n",
    ">** NOTE:**\n",
    "* **In the binary case with $X_i \\in \\{0, 1\\}$ ordering is not an issue in scikit-learn.**\n",
    "* **Ordering is relevant for features. Class labels for response $y$ are not ordinal for the ML algorithm we discuss in this course. Therefore it doesn't matter what number we assign them, and factorization is still valid (e.g. $y \\in \\{\\text{Product X}=0, \\text{Product Y}=1, \\text{Product Z}=2\\}$.**\n",
    "\n",
    "\n",
    "\n",
    "As soon as we have more than 2 categories, factorizing is in most cases no longer the appropriate solution. Instead, we should make use of a method called **one-hot encoding**, which effectively creates extra columns with dummy variables (booleans) indicating the presence of a category with a value of 1 or 0, respectively. There are numerous ways of encoding categorical values. In this chapter we will briefly touch upon Pandas' and Scikit-Learn's tools. For those looking for a thorough tutorial please refer to [Chris Moffit's (2017) excellent *Guide to Encoding Categorical Values in Python *](http://pbpython.com/categorical-encoding.html).\n",
    "\n",
    "\n",
    "#### get_dummies()\n",
    "\n",
    "We start with Pandas' `pd.get_dummies()` function, which works of course seamlessly with `DataFrames` and is truly easy to work with. The two caveats it has is that (a) if a column contains numbers, it will not transform them into categorical values and (b) this approach does not work in scikit-learn piepelines (Pipelines are a ML workflow management tool that we will discuss in more detail in later chapters.). Regarding (a) here's what is meant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dist</th>\n",
       "      <th>price</th>\n",
       "      <th>rooms</th>\n",
       "      <th>fab_bricks</th>\n",
       "      <th>fab_concrete</th>\n",
       "      <th>fab_wood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1390000</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1300000</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>840000</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1400000</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dist    price  rooms  fab_bricks  fab_concrete  fab_wood\n",
       "0     5  1390000    3.5           1             0         0\n",
       "1     2  1300000    4.5           0             1         0\n",
       "2    12   840000    3.5           0             0         1\n",
       "3     9  1400000    5.5           0             1         0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only the column 'fab', which contained strings, was converted. To make `get_dummies()` bend to our will, we have to convert the district values into strings first. Then we can apply the conversion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>rooms</th>\n",
       "      <th>dist_12</th>\n",
       "      <th>dist_2</th>\n",
       "      <th>dist_5</th>\n",
       "      <th>dist_9</th>\n",
       "      <th>fab_bricks</th>\n",
       "      <th>fab_concrete</th>\n",
       "      <th>fab_wood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1390000</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1300000</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>840000</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1400000</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     price  rooms  dist_12  dist_2  dist_5  dist_9  fab_bricks  fab_concrete  \\\n",
       "0  1390000    3.5        0       0       1       0           1             0   \n",
       "1  1300000    4.5        0       1       0       0           0             1   \n",
       "2   840000    3.5        1       0       0       0           0             0   \n",
       "3  1400000    5.5        0       0       0       1           0             1   \n",
       "\n",
       "   fab_wood  \n",
       "0         0  \n",
       "1         0  \n",
       "2         1  \n",
       "3         0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['dist'] = data['dist'].astype(str)\n",
    "pd.get_dummies(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LabelEncoder()\n",
    "\n",
    "The alternatives in Scikit-learn are called `LabelEncoder` (equivalent to `pd.facorize()`) and `OneHotEncoder()` (similar to `pd.get_dummies()`). Their setup is a bit more abstract and cumbersome, but it also has advantages compared to Pandas' solution. For example the `OneHotEncoder()` can be used in Scikit-learn pipelines and it returns a sparse matrix which is highly efficient computationally. But for the sake of brefity we will not go into too much detail here but simply show for reference how these Scikit-learn functions are applied. Ultimately it is at this stage a question of preference what functions you want to use for your task. Important is just that this step is done at the very beginning of the process - even before a train-test split is applied. That way you make sure the mapping is the same for both train and test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing as pp\n",
    "\n",
    "# Factorize 'fab' column (similar to pd.factorize())\n",
    "le = pp.LabelEncoder()\n",
    "data_le = le.fit_transform(data['fab'])\n",
    "data_le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert the integer class labels back into their original representation we can simply use the `.inverse_transform()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bricks', 'concrete', 'wood', 'concrete'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform(data_le)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OneHotEncoder\n",
    "\n",
    "Contrary to `pd.get_dummy()`, which works only on `strings`, `OneHotEncoder` can only be applied on categorical **integers**. Thus strings in the `fab` column have to be factorized first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x14 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 16 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['fab'] = data_le\n",
    "ohe = pp.OneHotEncoder(sparse=True)\n",
    "ohe.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output here is a sparse matrix. Computationally and from a data storage perspective this is highly efficient because the matrix contains only integers as entries. However, the print-out of this sparse matrix is less appealing. All column labels are empty. To make sense of it we have to understand the setup of this sparse matrix. Each column is a binary representation of a value. The first column indicates which of the rows had a `dist=2` (only row 1), second `dist=5` (only row 0), third `dist=9`, fourth `dist=12`, fifth `fab=0` etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0    1    2    3    4    5    6    7    8    9    10   11   12   13\n",
      "0  0.0  1.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0  1.0  0.0  0.0\n",
      "1  1.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0\n",
      "2  0.0  0.0  0.0  1.0  0.0  0.0  1.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0\n",
      "3  0.0  0.0  1.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  1.0\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(ohe.fit_transform(data).toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a short overview of the discussed functions and how they relate to each other:\n",
    "\n",
    "|No. of Categories | Applicable Functions |\n",
    "|:----------------:|:---------------------|\n",
    "| 2 | `pd.factorize(), sklearn.preprocessing.LabelEncoder()` |\n",
    "| > 2 | `pd.get_dummies(), sklearn.preprocessing.OneHotEncoder()`|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Incorrect class labeling is one of the more common mistake made in machine learning. The crux of the matter is that even if we forget to One-Hot-Encode our features an ML algorithm might still yield good results. Yet, as we know now, such results would be flawed. Therefore it is important to correctly preprocess the data set before applying any ML algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Partitioning a Data Set Into Train, Test Sets\n",
    "\n",
    "In section 4.5 of the script we have briefly touched upon the importance of randomly splitting our data into a training set (to train/calibrate our ML algorithms) and a test set (to evaluate the accuracy of our model). As [Pedregosa et al. (2011) put it in the scikit-learn documentation](http://scikit-learn.org/stable/modules/cross_validation.html): \"Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called overfitting. To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a test set `X_test, y_test`.\" The split ratio is somewhat arbitrary but literature suggests a range between 60:40 (train:test) and 80:20 for smaller samples and 90:10 to 99:1 for large sets (several thousands of observations) (Raschka (2015)). \n",
    "\n",
    "In its `model_selection` submodule scikit-learn offers a convenient function called `train_test_split` that randomly splits a data set into separate train and holdout sets. To show how this function is applied we first load the publicly available 'adult' data set. It includes 14 features from the 1994 US census that measure an individual's characteristics. The response vector is `income`. The prediction task is to determine whether a person earns over 50K a year. For more information see [the data description](https://archive.ics.uci.edu/ml/datasets/adult).  This is of course a fairly simple data set but will do for our purposes. Nevertheless, in reality related tasks such as this are fairly common for credit card companies, leasing firms or banks that need to verify an customer's application for a credit (card), leasing, loan etc. (One could design this either as regression task or classification task with an adequate number of classes).\n",
    "\n",
    "Before we apply the `train_test_split` function we convert categorical columns in a first step - as learned above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>native-country</th>\n",
       "      <th>sex</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>226802</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Private</td>\n",
       "      <td>11th</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>United-States</td>\n",
       "      <td>Male</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>89814</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>United-States</td>\n",
       "      <td>Male</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>336951</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>United-States</td>\n",
       "      <td>Male</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  fnlwgt  education-num  capital-gain  capital-loss  hours-per-week  \\\n",
       "0   25  226802              7             0             0              40   \n",
       "1   38   89814              9             0             0              50   \n",
       "2   28  336951             12             0             0              40   \n",
       "\n",
       "    workclass    education       marital-status          occupation  \\\n",
       "0     Private         11th        Never-married   Machine-op-inspct   \n",
       "1     Private      HS-grad   Married-civ-spouse     Farming-fishing   \n",
       "2   Local-gov   Assoc-acdm   Married-civ-spouse     Protective-serv   \n",
       "\n",
       "  relationship    race  native-country    sex  income  \n",
       "0    Own-child   Black   United-States   Male   <=50K  \n",
       "1      Husband   White   United-States   Male   <=50K  \n",
       "2      Husband   White   United-States   Male    >50K  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Data/adult.csv', sep=',')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first six columns are numeric values. The remaining columns are categorical information and thus need to be converted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>sex</th>\n",
       "      <th>workclass_ Federal-gov</th>\n",
       "      <th>workclass_ Local-gov</th>\n",
       "      <th>workclass_ Never-worked</th>\n",
       "      <th>...</th>\n",
       "      <th>native-country_ Puerto-Rico</th>\n",
       "      <th>native-country_ Scotland</th>\n",
       "      <th>native-country_ South</th>\n",
       "      <th>native-country_ Taiwan</th>\n",
       "      <th>native-country_ Thailand</th>\n",
       "      <th>native-country_ Trinadad&amp;Tobago</th>\n",
       "      <th>native-country_ United-States</th>\n",
       "      <th>native-country_ Vietnam</th>\n",
       "      <th>native-country_ Yugoslavia</th>\n",
       "      <th>native-country_unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>226802</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>89814</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>336951</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>160323</td>\n",
       "      <td>10</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>103497</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  fnlwgt  education-num  capital-gain  capital-loss  hours-per-week  \\\n",
       "0   25  226802              7             0             0              40   \n",
       "1   38   89814              9             0             0              50   \n",
       "2   28  336951             12             0             0              40   \n",
       "3   44  160323             10          7688             0              40   \n",
       "4   18  103497             10             0             0              30   \n",
       "\n",
       "   sex  workclass_ Federal-gov  workclass_ Local-gov  workclass_ Never-worked  \\\n",
       "0    0                       0                     0                        0   \n",
       "1    0                       0                     0                        0   \n",
       "2    0                       0                     1                        0   \n",
       "3    0                       0                     0                        0   \n",
       "4    1                       0                     0                        0   \n",
       "\n",
       "            ...            native-country_ Puerto-Rico  \\\n",
       "0           ...                                      0   \n",
       "1           ...                                      0   \n",
       "2           ...                                      0   \n",
       "3           ...                                      0   \n",
       "4           ...                                      0   \n",
       "\n",
       "   native-country_ Scotland  native-country_ South  native-country_ Taiwan  \\\n",
       "0                         0                      0                       0   \n",
       "1                         0                      0                       0   \n",
       "2                         0                      0                       0   \n",
       "3                         0                      0                       0   \n",
       "4                         0                      0                       0   \n",
       "\n",
       "   native-country_ Thailand  native-country_ Trinadad&Tobago  \\\n",
       "0                         0                                0   \n",
       "1                         0                                0   \n",
       "2                         0                                0   \n",
       "3                         0                                0   \n",
       "4                         0                                0   \n",
       "\n",
       "   native-country_ United-States  native-country_ Vietnam  \\\n",
       "0                              1                        0   \n",
       "1                              1                        0   \n",
       "2                              1                        0   \n",
       "3                              1                        0   \n",
       "4                              1                        0   \n",
       "\n",
       "   native-country_ Yugoslavia  native-country_unknown  \n",
       "0                           0                       0  \n",
       "1                           0                       0  \n",
       "2                           0                       0  \n",
       "3                           0                       0  \n",
       "4                           0                       0  \n",
       "\n",
       "[5 rows x 107 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get column names\n",
    "cols = df.columns.values[6:]\n",
    "\n",
    "# Factorize 'sex' and 'income' column (both binary)\n",
    "df[cols[-2:]] = df[cols[-2:]].apply(lambda x: pd.factorize(x)[0])\n",
    "\n",
    "# Assign response to y\n",
    "y = df[cols[-1]]\n",
    "\n",
    "# Factorize categorical values, assign output to X\n",
    "X = pd.get_dummies(df.iloc[:, :-1])\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48842, 107)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to perform the train-test split. For this example we use a test set size of 30%. The parameter `random_state=0` fixes the random split in a way such that results are reproducible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A stratified sample is one that maintains the proportion of values as in the original data set. If, for example, the response vector $y$ is a binary categorical variable with 25% zeros and 75% ones, `stratify=y` ensures that the random splits have 25% zeros and 75% ones too. Note that `stratify=y` does not mean `stratify=yes` but rather tells the function to take the categorical proportions from response vector `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Feature scaling is a crucial step in preparing data for ML applications. While some of the algorithms are invariant to the feature's scale (e.g. decision trees and random forests, which we will discuss in the next chapter), the majority of machine learning and optimization algorithms perform much better if features are scaled. The reasons are two-fold: for one, most ML algorithm have optimization functions to find the optimal coefficients/hyperparameter and these functions work more efficient on scaled values; for another, algorithms such as KNN, which use a distance measure, will put much more weight on features that have a larger scale than others (Raschka (2015), Müller and Guido (2017)). A good example for the latter is a data set that contains 'age' as well as 'income' variables. For a KNN model a difference in $1'000 salary is enormous compared to a difference of 10 years in age. Thus such a model would be driven by 'income' - and this would be contrary to our intuition (James et al. (2013)). \n",
    "\n",
    "There are two common approaches to bringing different features onto the same scale: **normalization and standardization**. Unfortunately these terms are often used quite loosely in different fields so that the meaning has be derived from the context they are mentioned (Raschka (2015)).\n",
    "\n",
    "\n",
    "### Normalization\n",
    "\n",
    "In general, normalization refers to the process of rescaling the features to a range of $[0, 1]$. This can be viewed as a special case of min-max scaling. In our context, we normalize a feature column $X_i$ by the following convention:\n",
    "\n",
    "$$\\begin{equation}\n",
    "X_{i}^{\\text{norm}} = \\frac{X_i - \\min(X_i)}{\\max(X_i) - \\min(X_i)}\n",
    "\\end{equation}$$\n",
    "\n",
    "With Scikit-learn this is applied as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler \n",
    "\n",
    "# Get cols to scale\n",
    "cols_scl = X.columns.values[:6]\n",
    "\n",
    "# Apply MinMaxScaler on continuous columns only\n",
    "mms = MinMaxScaler()\n",
    "X_train_norm = mms.fit_transform(X_train[cols_scl])  # fit & transform\n",
    "X_test_norm  = mms.transform(X_test[cols_scl])  # ONLY transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to point out that we fit the `MinMaxScaler` **only once on the training data**, not the test data. On the test data we only run the `.transform()` method. The parameters we get out of the training set (i.e. `mms.data_min_` and `mms.data_max_`) are then used to transform any new data (such as the holdout set). \n",
    "\n",
    "> **Why aren't we applying the scaling on the full data set, before we split the data into train and test set? This would not be a good idea because of one particular reason: we would \"contaminate\" our model with information about the test set. If we were to do it we run the risk of having a very good model for our train AND test data but this model would perform poorly on new data that previously was not part of any train or test set. The effect of such an approach would be poor generalization. By the way, the same is true for feature selection. This too should not be done on the full set but only on a training set.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization\n",
    "\n",
    "When we need values in a bounded interval, normalization via min-max scaling is a useful technique. Yet for many machine learning algorithms standardization is a more practical approach. The rationale behind it is that most linear models (such as logistic regression, LDA or support vector machines) initialize coefficients/parameters/weights to small random values close to 0. By standardizing our features, we center the values at mean 0 with standard deviation of 1. This makes the computations easier to learn the coefficients/parameters/weights. Beyond that, standardization does not distort useful statistical information about outliers and with that makes the algorithm less sensitive to them in contrast to min-max scaling (Raschka (2015)). We express the process of standarizing a feature column by the following equation:\n",
    "\n",
    "$$\\begin{equation}\n",
    "X_{i}^{\\text{std}} = \\frac{X_i - \\bar{X}_i}{\\sigma_{X_i}}\n",
    "\\end{equation}$$\n",
    "\n",
    "Here we follow the common notation that $\\bar{X}_i$ is the mean of vector $X_i$ and $\\sigma_{X_i}$ the vector's standard deviation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "# Apply StandardScaler on continuous columns only\n",
    "stdsc = StandardScaler()\n",
    "X_train_std = stdsc.fit_transform(X_train[cols_scl])  # fit & transform\n",
    "X_test_std  = stdsc.transform(X_test[cols_scl])  # ONLY transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we fit the `StandardScaler()` only on the train set. The parameters of this first step (`stdsc.mean_, stdsc.var_`) are then applied to the test set with the code `stdsc.transform(X_test)`. Note that the variance in `StandardScaler()` has a degree of freedom of 0 (biased estimator). This explains the difference if you compare `stdsc.var_` with `X_train.var()`. Use `X_train.var(ddof=1)` instead to get the same result as with the `StandardScaler`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Scaling Methods\n",
    "\n",
    "For our purposes, the above introduced two scaling methods are sufficient. However, this is not to say others are not helpful and unnecessary. Scikit-learn of course offers many more scaling procedures. For an overview, see [Scikit-learn's guide on the topic](http://scikit-learn.org/stable/modules/preprocessing.html). For those who like to see the effects of the different scaling methods in 2D plots, see Scikit-learn's tutorial [\"Compare the effect of different scalers on data with outliers\"](http://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py). Finally, Sebastian Raschka published a text [\"About Feature Scaling and Normalization\"](http://sebastianraschka.com/Articles/2014_about_feature_scaling.html) that not only discusses the different scaling methods but also shows the positive effect scaling can have on ML predictions/scores. Please give it a read to understand the importance of scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "### Model Evaluation\n",
    "\n",
    "To evaluate our models, we have discussed the necessity to split our data set into training and test set. For this we introduced the `train_test_split()` function. A ML algorithm is calibrated on the training set and then, to see how well our model generalizes to new, previously unseen data, applied to the test data. In this section we expand on aspects of evaluating a model by discussing cross validation (CV). This is a statistical method of performance evaluation that is more stable and thorough than a simple split into training and test set (Müller and Guido (2017)). \n",
    "\n",
    "\n",
    "#### $k$-Fold Cross Validation\n",
    "\n",
    "In $k$-fold CV, we randomly split the training data set into $k$ folds without replacement, where $k − 1$\n",
    "folds are used for the model training and the remaining fold is used for testing. This procedure is repeated $k$ times so that we obtain $k$ performance estimates. The performance of a ML algorithm is then simply the average of the performance for the $k$ different, individual folds. \n",
    "\n",
    "* **What is the benefit of $k$-Fold Cross Validation?** Performance estimates are less sensitive to the subpartitioning of the training data compared to a simple train/test split (Raschka (2015)). By applying it, we receive a more stable indication of a model's generalization performance. \n",
    "* **How many folds $k$ should we use?** The number of folds $k$ is usually 5 (large data sets) or 10 (small data sets) as suggested by Breiman and Spector (1992) or Kohavi (1995). \n",
    "\n",
    "In general, CV is best used in combination with hyperparameter tuning. Raschka (2015, p. 175) writes: \n",
    "\n",
    "> **\"Typically, we use k-fold cross-validation for model tuning, that is, finding the optimal hyperparameter values that yield a satisfying generalization performance. Once we have found satisfactory hyperparameter values, we can retrain the model on the complete training set and obtain a final performance estimate using the independent\n",
    "test set.\"**\n",
    "\n",
    "The following figure displays the concept of $k$-fold CV with $k=5$. The training data set is divided into 5\n",
    "folds. For each iteration 4 folds are used for training and 1 fold for model evaluation. The estimated performance $E$, this could be classification accuracy, an error rate etc., is calculated on the basis of the five performances $E_i$ (one for each iteration). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Graphics/0209_kFoldCV.png\" alt=\"kFoldCV\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> **Despite the fact that many text books are applying CV on the full data set, students should be aware that in order to be truly consistent, $k$-fold CV should essentially be applied to the training data only. Similar to our discussion on feature scaling, the argument is again that if we use the full available data set, our results are somewhat contaminated or distorted.** \n",
    "\n",
    "Below we show how to use Scikit-learn to perform $k$-fold CV. The data is our 'adult' set from before and the algorithm we apply is logistic regression. Note here that we will use Scikit-learn's implementation of logistic regression. We start with the `StratifiedKFold()` function. Stratifying here means that proportions between classes are the same in each fold. To evaluate classifiers this `StratifiedKFold()` function is usually preferred (over the simple `KFold()` function) because it results in more reliable estimates of generalization performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.81283813  0.81558935  0.8157356   0.81717127  0.81453854]\n",
      "CV accuracy on train set:  0.815 +/-  0.001\n"
     ]
    }
   ],
   "source": [
    "# Import necessary functions\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create k-Fold CV and LogReg object\n",
    "kFold = StratifiedKFold(n_splits=5, random_state=0)\n",
    "logReg = LogisticRegression()\n",
    "\n",
    "# Run CV and print results\n",
    "scores = cross_val_score(logReg, X_train_std, y_train, cv=kFold)\n",
    "print(scores)\n",
    "print('CV accuracy on train set: {0: .3f} +/- {1: .3f}'.format(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could shuffle the data instead of stratifying the folds. We do this by setting the `shuffle` parameter of `KFold` to `True`. The marginal difference is due to the different set up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.81529687  0.80476748  0.81324949  0.80973969  0.81366096]\n",
      "CV accuracy on train set:  0.811 +/-  0.004\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Create k-Fold CV\n",
    "kFold = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# Run CV and print results\n",
    "scores = cross_val_score(logReg, X_train_norm, y_train, cv=kFold)\n",
    "print(scores)\n",
    "print('CV accuracy on train set: {0: .3f} +/- {1: .3f}'.format(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a comparison, we can also run a test on the unscaled data set `X_train`. The results are indeed worse as literature suggests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV accuracy on train set:  0.799 +/-  0.002\n"
     ]
    }
   ],
   "source": [
    "# Run CV on unscaled values and print results\n",
    "scores = cross_val_score(logReg, X_train, y_train, cv=kFold)\n",
    "print('CV accuracy on train set: {0: .3f} +/- {1: .3f}'.format(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `cross_val_score` returns the accuracy (or score) of the model. Recall that this is simply the number of correctly classified samples. If we instead wish to have another output, we do this by providing a `scoring` parameter. Below is an example to call for `scoring='roc_auc'`, which will provide the ROC's area under the curve. The full list of available scoring parameter [can be found here](http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV AUC on train set:  0.829 +/-  0.003\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(logReg, X_train_std, y_train, cv=kFold, scoring='roc_auc')\n",
    "print('CV AUC on train set: {0: .3f} +/- {1: .3f}'.format(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noteworthy is one last function: `cross_validate`. This let's us define a list of measures that we can pass on to the function as scoring parameter. The output is a dictionary with `fit_time` and `score_time` (time elapsed to fit model/calculate scores), `train_accuracy` and `test_accuracy` (accuracy for the training set and validation set), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([ 0.31211162,  0.3419044 ,  0.36496758,  0.36797714,  0.29678726]),\n",
       " 'score_time': array([ 0.04011106,  0.06316853,  0.03709865,  0.04211211,  0.03609705]),\n",
       " 'test_accuracy': array([ 0.7999415 ,  0.79657795,  0.80140392,  0.79540801,  0.80035103]),\n",
       " 'test_recall': array([ 0.26046798,  0.24169742,  0.25645756,  0.27695057,  0.26937269]),\n",
       " 'test_roc_auc': array([ 0.57131012,  0.55770066,  0.58315468,  0.59893389,  0.60309325]),\n",
       " 'train_accuracy': array([ 0.79825235,  0.7985814 ,  0.79799642,  0.79938576,  0.79796724]),\n",
       " 'train_recall': array([ 0.26201007,  0.26651411,  0.26071701,  0.25499846,  0.2590389 ]),\n",
       " 'train_roc_auc': array([ 0.58388924,  0.57877596,  0.58006564,  0.56678831,  0.60680298])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "# Calculate return\n",
    "measures = ['accuracy', 'recall', 'roc_auc']\n",
    "scores = cross_validate(logReg, X_train, y_train, cv=kFold, scoring=measures, n_jobs=2)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set accuracy (CV=5):     0.798436635097\n",
      "Validation set scores (CV=5):  0.79873648292\n",
      "Test set accuracy:             0.813621783935\n"
     ]
    }
   ],
   "source": [
    "print('Train set accuracy (CV=5):    ', scores['train_accuracy'].mean())\n",
    "print('Validation set scores (CV=5): ', scores['test_accuracy'].mean())\n",
    "print('Test set accuracy:            ', logReg.fit(X_test_std, y_test).score(X_test_std, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A great feature available in all discussed CV functions is the possibility to set the number of CPUs to use to do the computation on. For this define `n_jobs=n`, where `n` is the number of CPU's you want to use. `n_jobs=-1` will use all available cores. This parallelization is especially helpful if you work on large data sets and/or computationally expensive tasks such as CV. If you want to know how many CPUs your computer runs on you can type the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leave-One-Out Cross Validation\n",
    "\n",
    "If we increase the number of folds to $n-1$ ($n$ = number of observations), that is, we train on all points but one in each trial, we call this Leave-One-Out CV or LOOCV. This can be seen as an extreme case of $k$-fold CV. On small data sets this might provide better estimates, but it can be very time consuming, particularly on larger data sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOCV accuracy on train set: 0.814969784106\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "# Create objects\n",
    "loocv = LeaveOneOut()\n",
    "#logReg = LogisticRegression()\n",
    "\n",
    "# Calculate & print scores\n",
    "scores = cross_val_score(logReg, X_train_std, y_train)\n",
    "print('LOOCV accuracy on train set:', np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Other Cross Validation Approaches\n",
    "\n",
    "While we limit ourselves to the two most known and used splitting strategies, Scikit-learn offers more than just the two presented here. A good overview is provided in the packages' [tutorial on cross validation](http://scikit-learn.org/stable/modules/cross_validation.html). However, as an introduction to the topic and given the use cases in this seminar the above two serve the purpose well.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Ressources\n",
    "\n",
    "\n",
    "In writing this notebook, many ressources were consulted. For internet ressources the links are provided within the textflow above and will therefore not be listed again. Beyond these links, the following ressources were consulted and are recommended as further reading on the discussed topics:\n",
    "\n",
    "* Breiman, Leo, and and Philip Spector, 1992, Submodel selection and Evaluation in Regression: the X-Random Case, *International Statistical Review* 60: 291–319.\n",
    "* Friedman, Jerome, Trevor Hastie, and Robert Tibshirani, 2001, *The Elements of Statistical Learning* (Springer, New York, NY).\n",
    "* James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani, 2013, *An Introduction to Statistical Learning: With Applications in R* (Springer Science & Business Media, New York, NY).\n",
    "* Kohavi, Ron, 1995, A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection, in *International Joint Conference on Artificial Intelligence (IJCAI)*, 1137-145, Stanford, CA\n",
    "* Müller, Andreas C., and Sarah Guido, 2017, *Introduction to Machine Learning with Python* (O’Reilly Media, Sebastopol, CA).\n",
    "* Raschka, Sebastian, 2015, *Python Machine Learning* (Packt Publishing Ltd., Birmingham, UK)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
